{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51cdc61-2797-499d-a149-09cd412a7f4d",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17206e3d-1ebe-4890-beba-c2da660efdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d46016-96f3-47a2-9634-401097c5efea",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154df47e-9243-47e3-92bb-f2a4a4ee5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your existing dataset\n",
    "df = pd.read_csv('Cisco-netflow-10.131.6.1-28Sept-to-02Oct-24.csv')  # Update the path to your dataset\n",
    "\n",
    "# Load known IPs from your CSV file\n",
    "known_ips_df = pd.read_csv('Known IP.csv')  # Update the path to your known IPs CSV file\n",
    "known_ips = known_ips_df['IP'].tolist()  # Assuming the column in the CSV is named 'IP'\n",
    "\n",
    "# Constants for labeling\n",
    "ABSTAIN = -1\n",
    "NORMAL = 0\n",
    "ANOMALY = 1\n",
    "\n",
    "# Define Labeling Functions (LFs) using Snorkel based on the anomaly rules\n",
    "\n",
    "@labeling_function()\n",
    "def lf_in_pkts(x):\n",
    "    if x['netflow.in_pkts'] < 3 or x['netflow.in_pkts'] > 1000:\n",
    "        return ANOMALY\n",
    "    return NORMAL\n",
    "\n",
    "@labeling_function()\n",
    "def lf_in_bytes(x):\n",
    "    if x['netflow.in_bytes'] > 100 * 1024 * 1024 or x['netflow.in_bytes'] == 500:  # > 100 MB or exactly 500 bytes\n",
    "        return ANOMALY\n",
    "    return NORMAL\n",
    "\n",
    "@labeling_function()\n",
    "def lf_protocol(x):\n",
    "    if x['netflow.protocol'] in [47, 50, 41]:  # GRE, ESP, IPv6 encapsulation\n",
    "        return ANOMALY\n",
    "    return NORMAL\n",
    "\n",
    "@labeling_function()\n",
    "def lf_l4_src_port(x):\n",
    "    if x['netflow.l4_src_port'] < 1024 or x['netflow.l4_src_port'] in [4444, 6667]:  # < 1024 or malware ports\n",
    "        return ANOMALY\n",
    "    return NORMAL\n",
    "\n",
    "@labeling_function()\n",
    "def lf_l4_dst_port(x):\n",
    "    if x['netflow.l4_dst_port'] not in range(33434, 33465):  # Outside Traceroute range\n",
    "        return ANOMALY\n",
    "    return NORMAL\n",
    "\n",
    "# Combine all LFs into a list\n",
    "lfs = [lf_in_pkts, lf_in_bytes, lf_protocol, lf_l4_src_port, lf_l4_dst_port]\n",
    "\n",
    "# Apply the LFs to the dataset\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df)\n",
    "\n",
    "# Analyze the outputs of the labeling functions\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()\n",
    "\n",
    "# Train a Label Model to combine the weak labels from the LFs\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100)\n",
    "\n",
    "# Predict probabilistic labels using the label model\n",
    "df['label'] = label_model.predict(L=L_train)\n",
    "\n",
    "# Separate normal and anomalous data\n",
    "df_normal = df[df['label'] == NORMAL]\n",
    "df_anomalous = df[df['label'] == ANOMALY]\n",
    "\n",
    "# Ensure we have the desired number of records\n",
    "normal_data_count = 500000\n",
    "anomaly_data_counts = [80000, 80000, 80000, 80000, 80000, 100000]  # 500,000 anomalies total\n",
    "\n",
    "# Step 1: Sampling 500,000 normal data points\n",
    "df_normal_sampled = df_normal.sample(n=normal_data_count, random_state=42)\n",
    "\n",
    "# Helper function to handle insufficient records and add synthetic anomalies if needed\n",
    "def safe_sample(df, n, rule_description):\n",
    "    if len(df) >= n:\n",
    "        return df.sample(n=n, random_state=42)\n",
    "    else:\n",
    "        print(f\"Not enough records for {rule_description}, only {len(df)} available. Adding synthetic anomalies.\")\n",
    "        additional_data = generate_synthetic_anomalies(n - len(df), rule_description)\n",
    "        return pd.concat([df, additional_data])\n",
    "\n",
    "# Function to generate synthetic anomalies with known IP addresses\n",
    "def generate_synthetic_anomalies(count, rule_description):\n",
    "    synthetic_data = []\n",
    "    for _ in range(count):\n",
    "        # Generate values for necessary fields\n",
    "        in_pkts = random.randint(1, 5000)\n",
    "        in_bytes = random.randint(500, 500 * 1024 * 1024)  # up to 500 MB\n",
    "        protocol = random.choice([6, 17, 47, 50, 41])  # Add anomalies for protocol\n",
    "        src_port = random.randint(1, 65535)\n",
    "        dst_port = random.randint(1, 65535)\n",
    "\n",
    "        # Select random known IPs from the known IP list\n",
    "        src_ip = random.choice(known_ips)\n",
    "        dst_ip = random.choice(known_ips)\n",
    "\n",
    "        # Generate timestamps\n",
    "        now = datetime.now()\n",
    "        timestamp = now.isoformat()  # Current timestamp\n",
    "        last_switched = now - timedelta(seconds=random.randint(1, 1000))\n",
    "        first_switched = last_switched - timedelta(seconds=random.randint(1, 1000))\n",
    "        \n",
    "        # Randomly generate TCP flags (common values like ACK, SYN-ACK)\n",
    "        tcp_flags = random.choice([24, 27])  # 24 = ACK, 27 = SYN-ACK\n",
    "        \n",
    "        synthetic_data.append({\n",
    "            '@timestamp': timestamp,\n",
    "            'netflow.in_pkts': in_pkts,\n",
    "            'netflow.in_bytes': in_bytes,\n",
    "            'netflow.protocol': protocol,\n",
    "            'netflow.l4_src_port': src_port,\n",
    "            'netflow.l4_dst_port': dst_port,\n",
    "            'netflow.ipv4_src_addr': src_ip,\n",
    "            'netflow.ipv4_dst_addr': dst_ip,\n",
    "            'netflow.first_switched': first_switched.isoformat(),\n",
    "            'netflow.last_switched': last_switched.isoformat(),\n",
    "            'netflow.tcp_flags': tcp_flags,\n",
    "            'label': ANOMALY  # Anomalous label\n",
    "        })\n",
    "    return pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Step 2: Sampling anomalous data based on the different anomaly rules\n",
    "\n",
    "# Rule-based sampling (for each rule and all rules combined)\n",
    "df_anomalous_in_pkts = safe_sample(df_anomalous[(df_anomalous['netflow.in_pkts'] < 3) | \n",
    "                                                 (df_anomalous['netflow.in_pkts'] > 1000)],\n",
    "                                   anomaly_data_counts[0], 'netflow.in_pkts')\n",
    "\n",
    "df_anomalous_in_bytes = safe_sample(df_anomalous[(df_anomalous['netflow.in_bytes'] > 100 * 1024 * 1024) | \n",
    "                                                  (df_anomalous['netflow.in_bytes'] == 500)],\n",
    "                                    anomaly_data_counts[1], 'netflow.in_bytes')\n",
    "\n",
    "df_anomalous_protocol = safe_sample(df_anomalous[df_anomalous['netflow.protocol'].isin([47, 50, 41])],\n",
    "                                    anomaly_data_counts[2], 'netflow.protocol')\n",
    "\n",
    "df_anomalous_src_port = safe_sample(df_anomalous[(df_anomalous['netflow.l4_src_port'] < 1024) | \n",
    "                                                  (df_anomalous['netflow.l4_src_port'].isin([4444, 6667]))],\n",
    "                                    anomaly_data_counts[3], 'netflow.l4_src_port')\n",
    "\n",
    "df_anomalous_dst_port = safe_sample(df_anomalous[~df_anomalous['netflow.l4_dst_port'].between(33434, 33464)],\n",
    "                                    anomaly_data_counts[4], 'netflow.l4_dst_port')\n",
    "\n",
    "# For the 100,000 anomalies combining all rules, we'll sample from the entire anomalous dataset\n",
    "df_anomalous_all_rules = safe_sample(df_anomalous, anomaly_data_counts[5], 'all rules')\n",
    "\n",
    "# Combine all the anomalous datasets\n",
    "df_anomalous_sampled = pd.concat([df_anomalous_in_pkts, df_anomalous_in_bytes, \n",
    "                                  df_anomalous_protocol, df_anomalous_src_port, \n",
    "                                  df_anomalous_dst_port, df_anomalous_all_rules])\n",
    "\n",
    "# Final dataset with 500,000 normal and 500,000 anomalous records\n",
    "df_final = pd.concat([df_normal_sampled, df_anomalous_sampled], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_final.to_csv('dataset-5.csv', index=False)\n",
    "\n",
    "# Display a sample of the final dataset\n",
    "print(df_final.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
